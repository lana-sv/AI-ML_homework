{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "me5LC32rM59I"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "data=pd.read_csv(\"~/datacsv/HousingData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "5uw7P0DQOQCT",
    "outputId": "1b3bdc31-4d6f-4436-ee5b-b6e33b32492c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.02985</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.430</td>\n",
       "      <td>58.7</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.12</td>\n",
       "      <td>5.21</td>\n",
       "      <td>28.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD  TAX  PTRATIO  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900    1  296     15.3   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671    2  242     17.8   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671    2  242     17.8   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622    3  222     18.7   \n",
       "5  0.02985   0.0   2.18   0.0  0.458  6.430  58.7  6.0622    3  222     18.7   \n",
       "\n",
       "        B  LSTAT  MEDV  \n",
       "0  396.90   4.98  24.0  \n",
       "1  396.90   9.14  21.6  \n",
       "2  392.83   4.03  34.7  \n",
       "3  394.63   2.94  33.4  \n",
       "5  394.12   5.21  28.7  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "ZnT1NpG_QnMZ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRIM       0\n",
       "ZN         0\n",
       "INDUS      0\n",
       "CHAS       0\n",
       "NOX        0\n",
       "RM         0\n",
       "AGE        0\n",
       "DIS        0\n",
       "RAD        0\n",
       "TAX        0\n",
       "PTRATIO    0\n",
       "B          0\n",
       "LSTAT      0\n",
       "MEDV       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "kcWwi5sPRS1N",
    "outputId": "4baadf4e-f916-49b1-f746-42c36188de70"
   },
   "outputs": [],
   "source": [
    "x = data.iloc[:,0:13].values\n",
    "y = data.iloc[:,13].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "WHclcA2OSUmV",
    "outputId": "f9b618f0-0756-4370-f726-c5ec067ec463"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.3200e-03, 1.8000e+01, 2.3100e+00, ..., 1.5300e+01, 3.9690e+02,\n",
       "        4.9800e+00],\n",
       "       [2.7310e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9690e+02,\n",
       "        9.1400e+00],\n",
       "       [2.7290e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9283e+02,\n",
       "        4.0300e+00],\n",
       "       ...,\n",
       "       [4.5270e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
       "        9.0800e+00],\n",
       "       [6.0760e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
       "        5.6400e+00],\n",
       "       [1.0959e-01, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9345e+02,\n",
       "        6.4800e+00]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24. , 21.6, 34.7, 33.4, 28.7, 27.1, 16.5, 15. , 18.9, 21.7, 20.4,\n",
       "       19.9, 23.1, 17.5, 20.2, 18.2, 13.6, 19.6, 15.2, 14.5, 15.6, 13.9,\n",
       "       16.6, 14.8, 18.4, 21. , 12.7, 14.5, 13.2, 13.1, 13.5, 21. , 24.7,\n",
       "       30.8, 34.9, 26.6, 25.3, 21.2, 19.3, 20. , 14.4, 19.4, 19.7, 25. ,\n",
       "       18.9, 35.4, 24.7, 31.6, 23.3, 19.6, 18.7, 16. , 22.2, 25. , 33. ,\n",
       "       23.5, 19.4, 22. , 17.4, 20.9, 24.2, 21.7, 22.8, 21.4, 20. , 20.8,\n",
       "       21.2, 28. , 23.9, 24.8, 22.9, 23.9, 26.6, 22.5, 23.6, 28.7, 22.6,\n",
       "       22. , 25. , 20.6, 28.4, 21.4, 38.7, 43.8, 33.2, 27.5, 26.5, 18.6,\n",
       "       20.1, 19.5, 19.5, 20.4, 19.8, 19.4, 21.7, 22.8, 18.8, 18.7, 18.5,\n",
       "       19.2, 22. , 20.3, 20.5, 18.8, 21.4, 16.2, 18. , 14.3, 19.2, 19.6,\n",
       "       23. , 15.6, 18.1, 17.4, 17.1, 17.8, 14. , 14.4, 13.4, 15.6, 11.8,\n",
       "       13.8, 15.4, 19.6, 19.4, 17. , 13.1, 24.3, 23.3, 27. , 50. , 50. ,\n",
       "       22.7, 25. , 50. , 23.8, 22.3, 17.4, 19.1, 23.1, 22.6, 29.4, 23.2,\n",
       "       29.9, 37.2, 39.8, 36.2, 37.9, 26.4, 29.6, 32. , 29.8, 34.9, 37. ,\n",
       "       29.1, 50. , 30.3, 34.6, 34.9, 32.9, 24.1, 42.3, 48.5, 50. , 24.4,\n",
       "       20. , 19.3, 22.4, 28.1, 23.7, 23.3, 28.7, 21.5, 26.7, 21.7, 27.5,\n",
       "       30.1, 44.8, 50. , 31.6, 24.3, 31.7, 41.7, 29. , 24. , 31.5, 23.3,\n",
       "       22.2, 23.7, 17.6, 24.3, 20.5, 24.5, 26.2, 24.4, 24.8, 29.6, 42.8,\n",
       "       20.9, 44. , 50. , 36. , 30.1, 33.8, 43.1, 31. , 36.5, 22.8, 50. ,\n",
       "       43.5, 20.7, 21.1, 24.4, 35.2, 32.4, 32. , 33.2, 29.1, 35.1, 45.4,\n",
       "       46. , 50. , 32.2, 22. , 23.2, 24.8, 28.5, 37.3, 23.9, 28.6, 27.1,\n",
       "       22.5, 29. , 24.8, 36.1, 33.4, 28.2, 22.8, 20.3, 16.1, 22.1, 19.4,\n",
       "       21.6, 23.8, 16.2, 19.8, 23.1, 21. , 23.8, 23.1, 20.4, 18.5, 25. ,\n",
       "       24.6, 23. , 22.2, 19.3, 22.6, 17.1, 22.2, 20.7, 21.1, 19.5, 18.5,\n",
       "       20.6, 19. , 18.7, 32.7, 16.5, 23.9, 17.5, 17.2, 23.1, 24.5, 24.1,\n",
       "       18.6, 30.1, 18.2, 17.8, 21.7, 22.7, 25. , 19.9, 20.8, 16.8, 21.9,\n",
       "       27.5, 21.9, 50. , 50. , 50. , 13.8, 13.8, 13.3, 13.1, 10.2, 10.4,\n",
       "       10.9, 11.3,  8.8,  7.2, 10.5,  7.4, 11.5, 15.1, 23.2,  9.7, 13.1,\n",
       "       12.5,  8.5,  5. ,  6.3,  5.6, 12.1,  8.3,  8.5,  5. , 17.2, 15. ,\n",
       "       17.2, 17.9, 16.3,  7. ,  7.2,  7.5, 10.4,  8.8,  8.4, 16.7, 14.2,\n",
       "       20.8, 11.7,  8.3, 11. , 14.1, 16.1, 14.3, 11.7, 13.4,  9.6,  8.7,\n",
       "        8.4, 12.8, 15.4, 10.8, 11.8, 14.9, 12.6, 14.1, 13. , 16.1, 14.9,\n",
       "       14.1, 12.7, 13.5, 20. , 17.7, 19.5, 20.2, 21.4, 19.1, 19.1, 20.1,\n",
       "       19.9, 19.6, 23.2, 13.8, 13.3, 16.7, 12. , 14.6, 23. , 23.7, 21.8,\n",
       "       20.6, 21.2, 19.1, 20.6, 15.2,  7. ,  8.1, 13.6, 20.1, 21.8, 24.5,\n",
       "       23.1, 19.7, 18.3, 21.2, 17.5, 16.8, 20.6, 23.9, 22. ])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "id": "g1NLN6q6ZnLh",
    "outputId": "567f87d5-8c07-4ccd-b597-402d12629bea"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.690136</td>\n",
       "      <td>11.460660</td>\n",
       "      <td>11.000863</td>\n",
       "      <td>0.068528</td>\n",
       "      <td>0.553215</td>\n",
       "      <td>6.280015</td>\n",
       "      <td>68.932741</td>\n",
       "      <td>3.805268</td>\n",
       "      <td>9.403553</td>\n",
       "      <td>406.431472</td>\n",
       "      <td>18.537563</td>\n",
       "      <td>358.490939</td>\n",
       "      <td>12.769112</td>\n",
       "      <td>22.359645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.202423</td>\n",
       "      <td>23.954082</td>\n",
       "      <td>6.908364</td>\n",
       "      <td>0.252971</td>\n",
       "      <td>0.113112</td>\n",
       "      <td>0.697985</td>\n",
       "      <td>27.888705</td>\n",
       "      <td>2.098571</td>\n",
       "      <td>8.633451</td>\n",
       "      <td>168.312419</td>\n",
       "      <td>2.166460</td>\n",
       "      <td>89.283295</td>\n",
       "      <td>7.308430</td>\n",
       "      <td>9.142979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.006320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.389000</td>\n",
       "      <td>3.561000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>1.129600</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>1.730000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.081955</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.130000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.453000</td>\n",
       "      <td>5.879250</td>\n",
       "      <td>45.475000</td>\n",
       "      <td>2.110100</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>280.250000</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>376.707500</td>\n",
       "      <td>7.125000</td>\n",
       "      <td>16.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.268880</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.560000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538000</td>\n",
       "      <td>6.201500</td>\n",
       "      <td>77.700000</td>\n",
       "      <td>3.199200</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>19.100000</td>\n",
       "      <td>392.190000</td>\n",
       "      <td>11.300000</td>\n",
       "      <td>21.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.435973</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.624000</td>\n",
       "      <td>6.605500</td>\n",
       "      <td>94.250000</td>\n",
       "      <td>5.116700</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>666.000000</td>\n",
       "      <td>20.200000</td>\n",
       "      <td>396.900000</td>\n",
       "      <td>17.117500</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>88.976200</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>27.740000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>8.780000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>12.126500</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>711.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>396.900000</td>\n",
       "      <td>37.970000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             CRIM          ZN       INDUS        CHAS         NOX          RM  \\\n",
       "count  394.000000  394.000000  394.000000  394.000000  394.000000  394.000000   \n",
       "mean     3.690136   11.460660   11.000863    0.068528    0.553215    6.280015   \n",
       "std      9.202423   23.954082    6.908364    0.252971    0.113112    0.697985   \n",
       "min      0.006320    0.000000    0.460000    0.000000    0.389000    3.561000   \n",
       "25%      0.081955    0.000000    5.130000    0.000000    0.453000    5.879250   \n",
       "50%      0.268880    0.000000    8.560000    0.000000    0.538000    6.201500   \n",
       "75%      3.435973   12.500000   18.100000    0.000000    0.624000    6.605500   \n",
       "max     88.976200  100.000000   27.740000    1.000000    0.871000    8.780000   \n",
       "\n",
       "              AGE         DIS         RAD         TAX     PTRATIO           B  \\\n",
       "count  394.000000  394.000000  394.000000  394.000000  394.000000  394.000000   \n",
       "mean    68.932741    3.805268    9.403553  406.431472   18.537563  358.490939   \n",
       "std     27.888705    2.098571    8.633451  168.312419    2.166460   89.283295   \n",
       "min      2.900000    1.129600    1.000000  187.000000   12.600000    2.600000   \n",
       "25%     45.475000    2.110100    4.000000  280.250000   17.400000  376.707500   \n",
       "50%     77.700000    3.199200    5.000000  330.000000   19.100000  392.190000   \n",
       "75%     94.250000    5.116700   24.000000  666.000000   20.200000  396.900000   \n",
       "max    100.000000   12.126500   24.000000  711.000000   22.000000  396.900000   \n",
       "\n",
       "            LSTAT        MEDV  \n",
       "count  394.000000  394.000000  \n",
       "mean    12.769112   22.359645  \n",
       "std      7.308430    9.142979  \n",
       "min      1.730000    5.000000  \n",
       "25%      7.125000   16.800000  \n",
       "50%     11.300000   21.050000  \n",
       "75%     17.117500   25.000000  \n",
       "max     37.970000   50.000000  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AlSistoQZs8_",
    "outputId": "dc69d722-4460-4c5a-83c7-0ab9ce59ed04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 394 entries, 0 to 504\n",
      "Data columns (total 14 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   CRIM     394 non-null    float64\n",
      " 1   ZN       394 non-null    float64\n",
      " 2   INDUS    394 non-null    float64\n",
      " 3   CHAS     394 non-null    float64\n",
      " 4   NOX      394 non-null    float64\n",
      " 5   RM       394 non-null    float64\n",
      " 6   AGE      394 non-null    float64\n",
      " 7   DIS      394 non-null    float64\n",
      " 8   RAD      394 non-null    int64  \n",
      " 9   TAX      394 non-null    int64  \n",
      " 10  PTRATIO  394 non-null    float64\n",
      " 11  B        394 non-null    float64\n",
      " 12  LSTAT    394 non-null    float64\n",
      " 13  MEDV     394 non-null    float64\n",
      "dtypes: float64(12), int64(2)\n",
      "memory usage: 46.2 KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b3rS4pWkaVRG",
    "outputId": "8bedb913-9c44-41ad-cfc4-5778e0cb8b44"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(394, 14)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V-pfL-7xaDta",
    "outputId": "1b2c4213-0884-42a9-c4a0-a7ec61f61303"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRIM       0\n",
       "ZN         0\n",
       "INDUS      0\n",
       "CHAS       0\n",
       "NOX        0\n",
       "RM         0\n",
       "AGE        0\n",
       "DIS        0\n",
       "RAD        0\n",
       "TAX        0\n",
       "PTRATIO    0\n",
       "B          0\n",
       "LSTAT      0\n",
       "MEDV       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p0YhJOdiRHqO",
    "outputId": "4fee1096-4eb1-465c-9e26-cab61b50772a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(315, 13) (79, 13) (315,) (79,)\n"
     ]
    }
   ],
   "source": [
    "# split into train test sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.68400e-02, 0.00000e+00, 3.41000e+00, ..., 1.78000e+01,\n",
       "        3.92180e+02, 8.81000e+00],\n",
       "       [5.26930e-01, 0.00000e+00, 6.20000e+00, ..., 1.74000e+01,\n",
       "        3.82000e+02, 4.63000e+00],\n",
       "       [4.42228e+00, 0.00000e+00, 1.81000e+01, ..., 2.02000e+01,\n",
       "        3.31290e+02, 2.13200e+01],\n",
       "       ...,\n",
       "       [1.20482e+01, 0.00000e+00, 1.81000e+01, ..., 2.02000e+01,\n",
       "        2.91550e+02, 1.41000e+01],\n",
       "       [6.44405e+00, 0.00000e+00, 1.81000e+01, ..., 2.02000e+01,\n",
       "        9.79500e+01, 1.20300e+01],\n",
       "       [1.30100e-02, 3.50000e+01, 1.52000e+00, ..., 1.55000e+01,\n",
       "        3.94740e+02, 5.49000e+00]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([22.6, 50. , 19.1, 21. , 14.9, 16.3, 21. , 12.7, 15. , 21.9, 24.8,\n",
       "       29. , 29.4, 16.5, 10.9,  8.5, 17.1, 19.8, 23.3,  6.3, 26.6, 34.7,\n",
       "       14.3, 20. , 20. , 21.6, 20.2, 23.1, 22.7, 20.6, 42.8, 20.2, 18.7,\n",
       "        9.7, 25. , 10.4, 11. , 14.1, 50. , 18.5, 20.8, 50. , 29.9, 14.4,\n",
       "       50. , 17.2, 19. , 21.1, 12.5, 29.1, 11.8, 16.2, 22.8, 21.4, 25. ,\n",
       "       22.8, 34.6, 19.5, 30.3, 24.4,  7.4, 14.9, 18.9, 19.5, 19.3, 36.5,\n",
       "       26.6,  9.6, 10.2, 23.1, 22.2, 19.6, 23. , 43.5, 30.1, 23.9, 24.3,\n",
       "       17.9, 28. ,  7.5, 17.5, 22.3, 29. , 19.1, 15.2, 26.2, 25. , 22.8,\n",
       "       25.3, 15.6, 21.4, 23.1, 24.6, 11.8, 20.6, 10.5, 24.8, 20. , 20.4,\n",
       "       21.2, 12. , 21.6, 23.7, 19.6, 22.6, 17.6, 19.3, 18.2, 14.3, 16.5,\n",
       "       19.4, 20.9, 16.1, 18.1, 21.2, 13.3, 16.6, 24.7, 17.4, 24.7, 50. ,\n",
       "       23.3, 17.8, 13.8, 18.3, 33.8, 24. , 50. , 28.6, 22.2, 50. , 24.5,\n",
       "       37. , 19.1, 23.8, 14.8, 11.3, 13.5, 13.8, 24.3, 32.4, 22.2, 15.6,\n",
       "       17.4, 16.1, 21.7, 13.1, 18.8, 22.5, 31.7, 31.6, 28.2, 20.6, 16. ,\n",
       "       36.1, 20.6, 37.9, 36. , 14. , 13.4, 44.8,  5.6, 34.9, 32.9, 19.6,\n",
       "       43.8, 12.6, 18.2, 21.1, 22.6,  7.2, 24.8, 22.5, 20.4, 27.5, 24. ,\n",
       "       13.2, 18.5, 21.7, 13.8, 24.5, 15.4, 29.8, 13.3, 24.5, 21.2, 20.3,\n",
       "       19.7, 14.6, 11.7, 34.9, 50. , 27.1, 13.8, 30.8, 20.9, 19.4, 15.4,\n",
       "       29.1, 22.1, 31.5, 23.3, 20.1, 28.7, 21. , 20.4, 41.7, 16.7, 14.5,\n",
       "       39.8, 37.3, 13.1, 16.7, 22.4, 18.6, 23.6, 24.8, 23.7, 20.1,  8.7,\n",
       "       19.4, 14.1, 24.3, 35.1, 23.2, 17.2, 27. , 25. , 18.5, 12.7,  8.1,\n",
       "       33. , 19.2, 50. , 33.4, 11.5, 28.7, 10.4,  5. ,  8.5, 33.2, 18.8,\n",
       "       22. , 30.1, 24.4, 28.7, 18. , 24.1, 42.3, 18.7, 35.2, 19.8, 21.7,\n",
       "       22.9, 29.6, 16.8, 13. , 34.9, 20.8, 21.7, 15.6, 19.9,  7. , 24.4,\n",
       "       13.1, 18.4, 22.8,  7.2, 20.5, 21.4, 17.7, 50. , 35.4, 23.8, 23.8,\n",
       "       23.1, 27.5, 22. , 17.5,  8.4, 14.4, 28.5, 33.4, 23.1, 15.2, 50. ,\n",
       "       23. , 23.1, 17.4, 19.9, 19.2, 20. , 23.7, 18.9,  7. , 18.7, 20.5,\n",
       "       13.4, 17. , 23.9, 26.7,  5. , 20.6, 25. , 20.7, 31. , 50. , 23.9,\n",
       "       19.5, 16.8, 28.4, 23.3, 20.8, 16.1, 32.7])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "mc=MinMaxScaler()\n",
    "x_train = mc.fit_transform( x_train)\n",
    "X_test = mc.fit_transform( x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "-_GxHzHERNUP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_6 (Dense)             (None, 104)               1456      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 104)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 52)                5460      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 52)                0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 26)                1378      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 26)                0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 27        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,321\n",
      "Trainable params: 8,321\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "\n",
    "# Initialize the model\n",
    "model=Sequential()\n",
    "\n",
    "# Build input layer & Hidden layer\n",
    "model.add(Dense(units=104, activation='relu', input_dim=13))\n",
    "model.add(Dropout(rate=0.1))\n",
    "\n",
    "# Building second layer\n",
    "model.add(Dense(units=52,activation ='relu'))\n",
    "model.add(Dropout(rate=0.1))\n",
    "\n",
    "# Building third layer\n",
    "model.add(Dense(units=26,activation ='relu'))\n",
    "model.add(Dropout(rate=0.1))\n",
    "\n",
    "#Adding outer layer\n",
    "model.add(Dense(units = 1, activation='linear'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='mae')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 22.0570 - val_loss: 21.2937\n",
      "Epoch 2/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 21.3276 - val_loss: 20.4489\n",
      "Epoch 3/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 20.3571 - val_loss: 19.1667\n",
      "Epoch 4/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 18.7333 - val_loss: 17.1962\n",
      "Epoch 5/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 16.3936 - val_loss: 14.4456\n",
      "Epoch 6/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 13.2202 - val_loss: 10.8673\n",
      "Epoch 7/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 9.9851 - val_loss: 8.4278\n",
      "Epoch 8/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 9.1877 - val_loss: 8.4330\n",
      "Epoch 9/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 9.1193 - val_loss: 7.7179\n",
      "Epoch 10/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 8.4724 - val_loss: 6.9501\n",
      "Epoch 11/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 7.8479 - val_loss: 6.6093\n",
      "Epoch 12/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 7.2806 - val_loss: 6.2237\n",
      "Epoch 13/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 7.2139 - val_loss: 5.7286\n",
      "Epoch 14/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 6.3804 - val_loss: 5.4969\n",
      "Epoch 15/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 6.3193 - val_loss: 5.3095\n",
      "Epoch 16/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 6.1165 - val_loss: 5.0665\n",
      "Epoch 17/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 6.1009 - val_loss: 4.8525\n",
      "Epoch 18/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 5.4116 - val_loss: 4.6992\n",
      "Epoch 19/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 5.4322 - val_loss: 4.5530\n",
      "Epoch 20/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 5.4830 - val_loss: 4.4542\n",
      "Epoch 21/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 5.1428 - val_loss: 4.3954\n",
      "Epoch 22/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 5.2942 - val_loss: 4.2795\n",
      "Epoch 23/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 5.1750 - val_loss: 4.2129\n",
      "Epoch 24/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 5.2817 - val_loss: 4.1334\n",
      "Epoch 25/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 5.1306 - val_loss: 4.0205\n",
      "Epoch 26/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 4.8733 - val_loss: 3.9569\n",
      "Epoch 27/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 5.1067 - val_loss: 4.0065\n",
      "Epoch 28/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 4.9903 - val_loss: 3.8837\n",
      "Epoch 29/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 4.8606 - val_loss: 3.7515\n",
      "Epoch 30/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 4.7747 - val_loss: 3.6924\n",
      "Epoch 31/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 4.3914 - val_loss: 3.6273\n",
      "Epoch 32/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 4.4165 - val_loss: 3.6979\n",
      "Epoch 33/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 4.4330 - val_loss: 3.5398\n",
      "Epoch 34/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 4.3751 - val_loss: 3.5053\n",
      "Epoch 35/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 4.6077 - val_loss: 3.4959\n",
      "Epoch 36/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 4.3948 - val_loss: 3.4549\n",
      "Epoch 37/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 4.2366 - val_loss: 3.5580\n",
      "Epoch 38/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 4.5958 - val_loss: 3.3212\n",
      "Epoch 39/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 4.3066 - val_loss: 3.2447\n",
      "Epoch 40/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 4.1708 - val_loss: 3.2877\n",
      "Epoch 41/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 4.2849 - val_loss: 3.1824\n",
      "Epoch 42/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 4.0293 - val_loss: 3.2034\n",
      "Epoch 43/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 4.0463 - val_loss: 3.1420\n",
      "Epoch 44/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.9138 - val_loss: 3.1678\n",
      "Epoch 45/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.8101 - val_loss: 3.1823\n",
      "Epoch 46/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.7637 - val_loss: 3.0487\n",
      "Epoch 47/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.6634 - val_loss: 3.0341\n",
      "Epoch 48/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 4.0730 - val_loss: 3.0634\n",
      "Epoch 49/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.8082 - val_loss: 3.1027\n",
      "Epoch 50/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 4.1139 - val_loss: 3.0005\n",
      "Epoch 51/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.9463 - val_loss: 2.9937\n",
      "Epoch 52/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.8559 - val_loss: 2.9826\n",
      "Epoch 53/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.9408 - val_loss: 2.9600\n",
      "Epoch 54/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.9534 - val_loss: 2.9929\n",
      "Epoch 55/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.7560 - val_loss: 2.9526\n",
      "Epoch 56/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.6502 - val_loss: 3.0913\n",
      "Epoch 57/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.6000 - val_loss: 2.9260\n",
      "Epoch 58/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.6154 - val_loss: 2.9409\n",
      "Epoch 59/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.7219 - val_loss: 2.9626\n",
      "Epoch 60/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.7673 - val_loss: 2.9770\n",
      "Epoch 61/200\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.7220 - val_loss: 3.0113\n",
      "Epoch 62/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.5826 - val_loss: 2.9443\n",
      "Epoch 63/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.6915 - val_loss: 2.9208\n",
      "Epoch 64/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.6575 - val_loss: 2.9421\n",
      "Epoch 65/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.3499 - val_loss: 2.9606\n",
      "Epoch 66/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.8203 - val_loss: 2.8869\n",
      "Epoch 67/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.6023 - val_loss: 2.8660\n",
      "Epoch 68/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.6901 - val_loss: 2.9399\n",
      "Epoch 69/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.3143 - val_loss: 2.8397\n",
      "Epoch 70/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.4595 - val_loss: 3.0535\n",
      "Epoch 71/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.3779 - val_loss: 2.8630\n",
      "Epoch 72/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.9352 - val_loss: 2.9133\n",
      "Epoch 73/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.5237 - val_loss: 2.8218\n",
      "Epoch 74/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.7247 - val_loss: 2.8183\n",
      "Epoch 75/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.5304 - val_loss: 2.9560\n",
      "Epoch 76/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.3577 - val_loss: 2.8253\n",
      "Epoch 77/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.5626 - val_loss: 2.8219\n",
      "Epoch 78/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.4411 - val_loss: 2.8854\n",
      "Epoch 79/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.3559 - val_loss: 2.7739\n",
      "Epoch 80/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.6797 - val_loss: 2.8515\n",
      "Epoch 81/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.5444 - val_loss: 2.7208\n",
      "Epoch 82/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.6262 - val_loss: 2.7134\n",
      "Epoch 83/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.8365 - val_loss: 2.7982\n",
      "Epoch 84/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.3398 - val_loss: 2.7214\n",
      "Epoch 85/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.2892 - val_loss: 2.8616\n",
      "Epoch 86/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.7544 - val_loss: 2.7169\n",
      "Epoch 87/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.3971 - val_loss: 2.7274\n",
      "Epoch 88/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.3633 - val_loss: 2.7469\n",
      "Epoch 89/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.4258 - val_loss: 2.7862\n",
      "Epoch 90/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.4834 - val_loss: 2.7968\n",
      "Epoch 91/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.5027 - val_loss: 2.9289\n",
      "Epoch 92/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.5608 - val_loss: 2.7903\n",
      "Epoch 93/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.6647 - val_loss: 2.8539\n",
      "Epoch 94/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.3969 - val_loss: 2.7517\n",
      "Epoch 95/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.4044 - val_loss: 2.8235\n",
      "Epoch 96/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.3882 - val_loss: 2.7484\n",
      "Epoch 97/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.6128 - val_loss: 2.6971\n",
      "Epoch 98/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.4940 - val_loss: 2.8122\n",
      "Epoch 99/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.1756 - val_loss: 2.6848\n",
      "Epoch 100/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.3506 - val_loss: 2.7729\n",
      "Epoch 101/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.4931 - val_loss: 2.7239\n",
      "Epoch 102/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.6080 - val_loss: 2.6766\n",
      "Epoch 103/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.4926 - val_loss: 2.7736\n",
      "Epoch 104/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.4030 - val_loss: 2.6790\n",
      "Epoch 105/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.3755 - val_loss: 2.7358\n",
      "Epoch 106/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.3488 - val_loss: 2.7039\n",
      "Epoch 107/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.4572 - val_loss: 2.7651\n",
      "Epoch 108/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.3215 - val_loss: 2.8873\n",
      "Epoch 109/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.5587 - val_loss: 2.7109\n",
      "Epoch 110/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.3559 - val_loss: 2.8340\n",
      "Epoch 111/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.4345 - val_loss: 2.8715\n",
      "Epoch 112/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.4052 - val_loss: 2.7461\n",
      "Epoch 113/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.5061 - val_loss: 2.7272\n",
      "Epoch 114/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.1504 - val_loss: 2.7696\n",
      "Epoch 115/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.3426 - val_loss: 2.7486\n",
      "Epoch 116/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3.4373 - val_loss: 2.7084\n",
      "Epoch 117/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.3474 - val_loss: 2.8148\n",
      "Epoch 118/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.3197 - val_loss: 2.7053\n",
      "Epoch 119/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.1158 - val_loss: 2.8232\n",
      "Epoch 120/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.2984 - val_loss: 2.7018\n",
      "Epoch 121/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.5206 - val_loss: 2.8862\n",
      "Epoch 122/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.3490 - val_loss: 2.6922\n",
      "Epoch 123/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.3720 - val_loss: 3.0102\n",
      "Epoch 124/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.5677 - val_loss: 2.7006\n",
      "Epoch 125/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.6954 - val_loss: 3.0277\n",
      "Epoch 126/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.5274 - val_loss: 2.6629\n",
      "Epoch 127/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.6914 - val_loss: 2.6341\n",
      "Epoch 128/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.4981 - val_loss: 2.7353\n",
      "Epoch 129/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.2154 - val_loss: 2.7049\n",
      "Epoch 130/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.4456 - val_loss: 2.6989\n",
      "Epoch 131/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.3566 - val_loss: 2.7853\n",
      "Epoch 132/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.3005 - val_loss: 2.6783\n",
      "Epoch 133/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.4399 - val_loss: 2.7592\n",
      "Epoch 134/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.4446 - val_loss: 2.6963\n",
      "Epoch 135/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.3427 - val_loss: 2.6509\n",
      "Epoch 136/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.3012 - val_loss: 2.8694\n",
      "Epoch 137/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.3721 - val_loss: 2.6681\n",
      "Epoch 138/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.5633 - val_loss: 2.6932\n",
      "Epoch 139/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.3608 - val_loss: 2.6507\n",
      "Epoch 140/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.3491 - val_loss: 2.6522\n",
      "Epoch 141/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.2399 - val_loss: 2.7813\n",
      "Epoch 142/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.3492 - val_loss: 2.5816\n",
      "Epoch 143/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.3559 - val_loss: 2.8578\n",
      "Epoch 144/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.5194 - val_loss: 2.7096\n",
      "Epoch 145/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.4399 - val_loss: 2.6508\n",
      "Epoch 146/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.3555 - val_loss: 2.7458\n",
      "Epoch 147/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.3916 - val_loss: 2.5433\n",
      "Epoch 148/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.4168 - val_loss: 2.6771\n",
      "Epoch 149/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.0919 - val_loss: 2.5788\n",
      "Epoch 150/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.5527 - val_loss: 2.6703\n",
      "Epoch 151/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.4916 - val_loss: 2.6489\n",
      "Epoch 152/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.2199 - val_loss: 2.6971\n",
      "Epoch 153/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.4910 - val_loss: 2.5495\n",
      "Epoch 154/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.1899 - val_loss: 2.5288\n",
      "Epoch 155/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.3789 - val_loss: 2.5469\n",
      "Epoch 156/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.2219 - val_loss: 2.5368\n",
      "Epoch 157/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.3685 - val_loss: 2.5152\n",
      "Epoch 158/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.4028 - val_loss: 2.7237\n",
      "Epoch 159/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3.3850 - val_loss: 2.4788\n",
      "Epoch 160/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.2012 - val_loss: 2.5019\n",
      "Epoch 161/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.1270 - val_loss: 2.5137\n",
      "Epoch 162/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.2169 - val_loss: 2.5718\n",
      "Epoch 163/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.2722 - val_loss: 2.5991\n",
      "Epoch 164/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 2.9298 - val_loss: 2.4558\n",
      "Epoch 165/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.0867 - val_loss: 2.8265\n",
      "Epoch 166/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.5286 - val_loss: 2.6549\n",
      "Epoch 167/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.2260 - val_loss: 2.4410\n",
      "Epoch 168/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.3782 - val_loss: 2.6753\n",
      "Epoch 169/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.3280 - val_loss: 2.4294\n",
      "Epoch 170/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.0027 - val_loss: 2.6943\n",
      "Epoch 171/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.3796 - val_loss: 2.5086\n",
      "Epoch 172/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.2737 - val_loss: 2.6452\n",
      "Epoch 173/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.2280 - val_loss: 2.5641\n",
      "Epoch 174/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.2888 - val_loss: 2.6493\n",
      "Epoch 175/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.2767 - val_loss: 2.5347\n",
      "Epoch 176/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.1926 - val_loss: 2.5310\n",
      "Epoch 177/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 2.9750 - val_loss: 2.6713\n",
      "Epoch 178/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 2.9371 - val_loss: 2.5399\n",
      "Epoch 179/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.4245 - val_loss: 2.6750\n",
      "Epoch 180/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.2709 - val_loss: 2.5482\n",
      "Epoch 181/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.3378 - val_loss: 2.7403\n",
      "Epoch 182/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.3402 - val_loss: 2.5152\n",
      "Epoch 183/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.0264 - val_loss: 2.8381\n",
      "Epoch 184/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.3250 - val_loss: 2.5004\n",
      "Epoch 185/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.2050 - val_loss: 2.6097\n",
      "Epoch 186/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.2570 - val_loss: 2.5058\n",
      "Epoch 187/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.5027 - val_loss: 2.4197\n",
      "Epoch 188/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.4169 - val_loss: 2.4168\n",
      "Epoch 189/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.3394 - val_loss: 2.6385\n",
      "Epoch 190/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.2096 - val_loss: 2.4422\n",
      "Epoch 191/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.3724 - val_loss: 2.5868\n",
      "Epoch 192/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.1538 - val_loss: 2.5755\n",
      "Epoch 193/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.2145 - val_loss: 2.4791\n",
      "Epoch 194/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 2.9604 - val_loss: 2.5579\n",
      "Epoch 195/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.1345 - val_loss: 2.6739\n",
      "Epoch 196/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3.2642 - val_loss: 2.6628\n",
      "Epoch 197/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.2589 - val_loss: 2.6175\n",
      "Epoch 198/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.1705 - val_loss: 2.4549\n",
      "Epoch 199/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.2843 - val_loss: 2.5145\n",
      "Epoch 200/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.3458 - val_loss: 2.5800\n"
     ]
    }
   ],
   "source": [
    "#Fit model:\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping_minitor = EarlyStopping(patience=200)\n",
    "\n",
    "#train model:\n",
    "history = model.fit(x_train, y_train,\n",
    " epochs=200,\n",
    " batch_size=32,\n",
    " validation_split=0.2,\n",
    " callbacks=[early_stopping_minitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "7w2nvTr7Yg00"
   },
   "outputs": [],
   "source": [
    "#model.fit(X_train, y_train,epochs=100,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'val_loss'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot loss of train and test set\n",
    "print(history.history.keys())\n",
    "\n",
    "#Loss in train and test:\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model_loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on test data\n",
      "10/10 [==============================] - 0s 665us/step - loss: 2.1455\n",
      "mae:  2.1455142498016357\n"
     ]
    }
   ],
   "source": [
    "#evaluate the result\n",
    "print('Evaluation on test data')\n",
    "results = model.evaluate(x_train, y_train)\n",
    "print('mae: ', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    394.000000\n",
       "mean      22.359645\n",
       "std        9.142979\n",
       "min        5.000000\n",
       "25%       16.800000\n",
       "50%       21.050000\n",
       "75%       25.000000\n",
       "max       50.000000\n",
       "Name: MEDV, dtype: float64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.MEDV.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09395973154362416"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mae/mean:  error is 9 precent\n",
    "\n",
    "2.1/22.35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RTyXSXX8czmO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
